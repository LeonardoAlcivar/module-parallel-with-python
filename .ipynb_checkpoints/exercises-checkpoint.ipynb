{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU-Bound Work\n",
    "\n",
    "To explore cpu-bound work, we will practise bitcoin mining! \n",
    "\n",
    "Hashing is a great example of cpu-bound work, and searching for a \"nonce\" in order to get a hash that looks a certain way is naturally cpu intensive!\n",
    "\n",
    "The function get_target gets passed a string, and then proceeds to do some slow work: hashing the string over and over, with a new \"none\" concatenated to the front, until it gets a hash that starts with the correct number of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha256\n",
    "\n",
    "def hash(s, nonce):\n",
    "    data = (str(nonce) + s).encode()\n",
    "    m = sha256()\n",
    "    m.update(data)\n",
    "    return m.hexdigest()\n",
    "\n",
    "def get_target(s, target = '00000'):\n",
    "    first = None\n",
    "    i = 0\n",
    "    while first != target:\n",
    "        h = hash(s, i)\n",
    "        first = h[:len(target)]\n",
    "        i += 1\n",
    "    return h, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Zen of Python\n",
    "\n",
    "We won't parallelize the actual mining, instead we will pretend that we need to hash multiple strings in this way, making the problem trivially parallelizable. The strings we will hash are the lines from the poem: The Zen of Python.\n",
    "\n",
    "The Zen of Python has been an easter egg in python for a very long time. When you \"import this\", the module prints out the poem. We will need to do a little trickery to get the poem in string form. You can see the module here: https://github.com/python/cpython/blob/3.7/Lib/this.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from this import s, d\n",
    "\n",
    "zen = \"\".join([d.get(c, c) for c in s])\n",
    "zens = [i for i in zen.split('\\n') if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell does everything we want. \n",
    "# But it's slow! Because it has not been parallelized.\n",
    "\n",
    "[get_target(z) for z in zens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "# Let's parallelize the process from the previous cell!\n",
    "# Use an ExecutorPool from the concurrent.futures module\n",
    "# Look up the documentation!\n",
    "# There are two types of pools: ProcessPoolExecutor \n",
    "# and ThreadPoolExecutor. Try both and time the execution!\n",
    "# Which is faster? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O-Bound Work\n",
    "\n",
    "Let's move on to I/O work, like in scraping! \n",
    "\n",
    "We know we should use a Thread Pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your previous code from the \"scraping\" module\n",
    "# Modify it such that, rather than returning the \"title\" of\n",
    "# each lego set, you return the URL (href attribute) to the\n",
    "# \"product page\" of the lego (the page you get to when you click\n",
    "# on the title)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should now have a list with several hundred different links, \n",
    "# each leading to the pages of different lego sets. \n",
    "\n",
    "# Here's a new function that scrapes the product page: \n",
    "def product_page(soup):\n",
    "    pass\n",
    "\n",
    "# Call this function in parallel, such that you retrieve \n",
    "# all the _______ from each product page.\n",
    "# Should this be with a ThreadPoolExecutor or a ProcessPoolExecutor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queues\n",
    "\n",
    "Unfortunately, scraping is not so simple. We rarely have a list of pages ahead-of-time, and it is rarely very efficient to collect the pages first, then scrape them. \n",
    "\n",
    "What we want is some way in which the list (of urls that we map over) can continually grow.\n",
    "\n",
    "That's what queues are for!\n",
    "\n",
    "Implement a queue... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
